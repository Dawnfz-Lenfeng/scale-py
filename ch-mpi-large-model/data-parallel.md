数据并行性是一种允许您通过在多个计算节点之间复制模型并在它们之间划分数据集来更快地训练模型。

首先对于一次顺序训练，其中我们有一个计算节点，该节点将我们的模型加载到内存中。在每次训练迭代期间，我们加载下一个小批量，并在缓存每层输出的同时对模型执行前向传递。然后，我们计算损失并运行向后传递，从而计算梯度。这个过程如下图所示，使用 MNIST 图像作为我们的示例输入数据。
![[数据并行图1.png]]
在数据并行（DataParallel）中，需要再N台机器上赋值模型，我们分割我们的数据为N个块，并且让每台机器处理一个块。![[数据并行图2.png]]对于整个过程，首先在初始对于两个计算节点，需要将模型参数scatter到进程组中的其他进程，之后对于每个计算节点，一旦计算出梯度，都需要进行MPI.AllReduce,收集全部的信息来更新参数。
![[数据并行图3.png]]
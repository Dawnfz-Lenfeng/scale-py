{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(mpi-collective)=\n",
    "# Collective Communication\n",
    "\n",
    "In the {ref}`mpi-point2point` section, we discussed point-to-point communication, which involves the mutual transfer of data between sender and receiver. This section focuses on a different type of communication â€“ collective communication, where data is simultaneously transmitted among multiple processes within a group. Collective communication only supports blocking modes.\n",
    "\n",
    "The commonly used collective communications include:\n",
    "\n",
    "* Synchronization: For example, [`Comm.Barrier`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Barrier).\n",
    "* Data Movement: Such as [`Comm.Bcast`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Bcast), [`Comm.Scatter`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Scatter), [`Comm.Gather`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Gather), etc.\n",
    "* Collective Computation: Including [`Comm.Reduce`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Reduce), [`Intracomm.Scan`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Intracomm.html#mpi4py.MPI.Intracomm.Scan), etc.\n",
    "\n",
    "Functions with uppercase initial letters are based on buffers, such as `Comm.Bcast`, `Comm.Scatter`, `Comm.Gather`, [`Comm.Allgather`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Allgather), [`Comm.Alltoall`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.Alltoall). Functions with lowercase initial letters can send and receive Python objects, such as [`Comm.bcast`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.bcast), [`Comm.scatter`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.scatter), [`Comm.gather`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.gather), [`Comm.allgather`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.allgather), [`Comm.alltoall`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Comm.html#mpi4py.MPI.Comm.alltoall).\n",
    "\n",
    "## Synchronization\n",
    "\n",
    "MPI computations are distributed across multiple processes, each with varying computational speeds. `Comm.Barrier` forces synchronization, as the name suggests, by setting up a barrier for all processes within the communicator. Faster processes reaching `Comm.Barrier` cannot proceed with the subsequent code logic until all other processes have also reached this point. It acts as a synchronization point, ensuring that all processes complete their computations before moving forward.\n",
    "\n",
    "## Data Movement\n",
    "\n",
    "### Broadcast\n",
    "\n",
    "`Comm.Bcast` globally broadcasts data from one sender to all processes within the group. Broadcast operations are useful in scenarios where the same data needs to be sent to all processes, such as broadcasting the value of a global variable to all processes, as illustrated in {numref}`mpi-broadcast`.\n",
    "\n",
    "```{figure} ../img/ch-mpi/broadcast.svg\n",
    "---\n",
    "width: 600px\n",
    "name: mpi-broadcast\n",
    "---\n",
    "Broadcast\n",
    "```\n",
    "\n",
    "### Example 1: Broadcast\n",
    "\n",
    "The example in {numref}`mpi-broadcast-py` demonstrates how to broadcast a NumPy array to all processes.\n",
    "\n",
    "```{code-block} python\n",
    ":caption: broadcast.py\n",
    ":name: mpi-broadcast-py\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "comm.Barrier()\n",
    "\n",
    "N = 5\n",
    "if comm.rank == 0:\n",
    "    A = np.arange(N, dtype=np.float64)    # rank 0 initializes data into variable A\n",
    "else:\n",
    "    A = np.empty(N, dtype=np.float64)     # As on other processes are empty\n",
    "\n",
    "# Broadcast\n",
    "comm.Bcast([A, MPI.DOUBLE])\n",
    "\n",
    "# Print to verify\n",
    "print(\"Rank:%2d, data:%s\" % (comm.rank, A))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 0, data:[0. 1. 2. 3. 4.]\n",
      "Rank: 2, data:[0. 1. 2. 3. 4.]\n",
      "Rank: 1, data:[0. 1. 2. 3. 4.]\n",
      "Rank: 3, data:[0. 1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -np 4 python broadcast.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter and Gather\n",
    "\n",
    "`Comm.Scatter` and `Comm.Gather` are a pair of corresponding operations:\n",
    "\n",
    "* `Comm.Scatter` scatters data from one process to all processes within the group. A process divides the data into multiple chunks, with each chunk sent to the corresponding process. Other processes receive and store their respective chunks. Scatter operations are suitable for partitioning a larger dataset into multiple smaller chunks.\n",
    "\n",
    "* `Comm.Gather`, on the contrary, gathers small data chunks from all processes in the group to one process.\n",
    "\n",
    "```{figure} ../img/ch-mpi/scatter-gather.svg\n",
    "---\n",
    "width: 600px\n",
    "name: mpi-scatter-gather\n",
    "---\n",
    "Scatter and Gather\n",
    "```\n",
    "\n",
    "### Example 2: Scatter\n",
    "\n",
    "The example in {numref}`mpi-scatter` demonstrates how to use Scatter to distribute data to all processes.\n",
    "\n",
    "```{code-block} python\n",
    ":caption: scatter.py\n",
    ":name: mpi-scatter\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "sendbuf = None\n",
    "if rank == 0:\n",
    "    sendbuf = np.empty([size, 8], dtype='i')\n",
    "    sendbuf.T[:,:] = range(size)\n",
    "    print(f\"Rank: {rank}, to be scattered: \\n{sendbuf}\")\n",
    "recvbuf = np.empty(8, dtype='i')\n",
    "comm.Scatter(sendbuf, recvbuf, root=0)\n",
    "print(f\"Rank: {rank}, after scatter: {recvbuf}\")\n",
    "assert np.allclose(recvbuf, rank)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 0, to be scattered: \n",
      "[[0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1]\n",
      " [2 2 2 2 2 2 2 2]\n",
      " [3 3 3 3 3 3 3 3]]\n",
      "Rank: 0, after scatter: [0 0 0 0 0 0 0 0]\n",
      "Rank: 1, after scatter: [1 1 1 1 1 1 1 1]\n",
      "Rank: 2, after scatter: [2 2 2 2 2 2 2 2]\n",
      "Rank: 3, after scatter: [3 3 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -np 4 python scatter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allgather and Alltoall\n",
    "\n",
    "Two more complex operations are `Comm.Allgather` and `Comm.Alltoall`.\n",
    "\n",
    "`Comm.Allgather` is an advanced version of `Comm.Gather`, as shown in {numref}`mpi-allgather`. It sends multiple small data chunks scattered across various processes to every process, ensuring that each process contains an identical set of data.\n",
    "\n",
    "```{figure} ../img/ch-mpi/allgather.svg\n",
    "---\n",
    "width: 600px\n",
    "name: mpi-allgather\n",
    "---\n",
    "Allgather\n",
    "```\n",
    "\n",
    "\n",
    "`Comm.Alltoall` is a combination of `Comm.Scatter` and `Comm.Gather`, as illustrated in {numref}`mpi-alltoall`. It first performs `Comm.Scatter` and then follows with `Comm.Gather`. If the data is viewed as a matrix, `Comm.Alltoall` can be considered a global transpose operation.\n",
    "\n",
    "```{figure} ../img/ch-mpi/alltoall.svg\n",
    "---\n",
    "width: 600px\n",
    "name: mpi-alltoall\n",
    "---\n",
    "Alltoall\n",
    "```\n",
    "\n",
    "## Collective Computation\n",
    "\n",
    "Collective computation refers to performing computations on data when aggregating scattered data from different processes, such as `Comm.Reduce` and [`Intracomm`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.Intracomm.html). As shown in {numref}`mpi-reduce` and {numref}`mpi-scan`, when data is gathered to a specific process, an aggregation function `f` is applied. Common aggregation functions include summation [`MPI.SUM`](https://mpi4py.readthedocs.io/en/stable/reference/mpi4py.MPI.SUM.html) and others.\n",
    "\n",
    "```{figure} ../img/ch-mpi/reduce.svg\n",
    "---\n",
    "width: 500px\n",
    "name: mpi-reduce\n",
    "---\n",
    "Reduce\n",
    "```\n",
    "\n",
    "```{figure} ../img/ch-mpi/scan.svg\n",
    "---\n",
    "width: 500px\n",
    "name: mpi-scan\n",
    "---\n",
    "Scan\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dispy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

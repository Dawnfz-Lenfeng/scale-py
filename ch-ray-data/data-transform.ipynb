{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ray-data-transform)=\n",
    "# 数据转换\n",
    "\n",
    "数据处理的核心在于对数据进行一系列的转换（Transform），本节将介绍：\n",
    "\n",
    "* 如何对一条、一批次进行转换\n",
    "* 如何进行分组 `groupby`\n",
    "* 如何随机洗牌\n",
    "\n",
    "## 转换\n",
    "\n",
    "### map() 与 map_batches()\n",
    "\n",
    "Ray Data 提供了两类数据转换操作，如 {numref}`map-map-batches` 所示：\n",
    "\n",
    "* 每行数据，可以用 `Dataset.map()` 和 `Dataset.flat_map()` 这两个 API，即对每一条数据一一进行转换。这与其他大数据框架（Spark 或者 Flink）类似。输入一条数据，输出一条数据。\n",
    "* 将多行数据打包为一个批次（Batch），对一个批次的数据进行转换：[`Dataset.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html)。输入一个 Batch，输出一个 Batch。\n",
    "\n",
    "```{figure} ../img/ch-ray-air/map-map-batches.svg\n",
    "---\n",
    "width: 800px\n",
    "name: map-map-batches\n",
    "---\n",
    "map() v.s. map_batches()\n",
    "```\n",
    "\n",
    "我们仍以纽约出租车数据为例，演示如何使用这两类转换操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 11:50:04,284\tINFO worker.py:1673 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件夹 /home/dispy/distributed-python/ch-ray-data/../data/nyc-taxi 已存在，无需操作。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import urllib.request\n",
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ray\n",
    "\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "\n",
    "ray.init()\n",
    "\n",
    "folder_path = os.path.join(os.getcwd(), \"../data/nyc-taxi\")\n",
    "download_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-06.parquet\"\n",
    "file_name = download_url.split(\"/\")[-1]\n",
    "parquet_file_path = os.path.join(folder_path, file_name)\n",
    "if not os.path.exists(folder_path):\n",
    "    # 创建文件夹\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"文件夹 {folder_path} 不存在，已创建。\")\n",
    "    # 下载并保存 Parquet 文件\n",
    "    with urllib.request.urlopen(download_url) as response, open(parquet_file_path, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)\n",
    "    print(\"数据已下载并保存为 Parquet 文件。\")\n",
    "else:\n",
    "    print(f\"文件夹 {folder_path} 已存在，无需操作。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据到 `Dataset` 类，先查看原有的数据格式，其中 `tpep_pickup_datetime` 和 `tpep_dropoff_datetime` 分别为乘客上车和下车时间，包含了日期和时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb2d08ada7644e29fdf346eab7ee4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=251327) Parquet Files Sample 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 11:50:13,760\tINFO dataset.py:2383 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2023-12-15 11:50:13,769\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2023-12-15 11:50:13,770\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 200, each read task output is split into 200 smaller blocks.\n",
      "2023-12-15 11:50:13,772\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> LimitOperator[limit=1]\n",
      "2023-12-15 11:50:13,773\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-15 11:50:13,774\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afc85fadce9463e951c5769d98ad0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=251327)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/arrow_block.py:128: FutureWarning: promote has been superseded by mode='default'.\n",
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=251327)\u001b[0m   return transform_pyarrow.concat(tables)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'VendorID': 1,\n",
       "  'tpep_pickup_datetime': datetime.datetime(2023, 6, 1, 0, 8, 48),\n",
       "  'tpep_dropoff_datetime': datetime.datetime(2023, 6, 1, 0, 29, 41),\n",
       "  'passenger_count': 1,\n",
       "  'trip_distance': 3.4,\n",
       "  'RatecodeID': 1,\n",
       "  'store_and_fwd_flag': 'N',\n",
       "  'PULocationID': 140,\n",
       "  'DOLocationID': 238,\n",
       "  'payment_type': 1,\n",
       "  'fare_amount': 21.9,\n",
       "  'extra': 3.5,\n",
       "  'mta_tax': 0.5,\n",
       "  'tip_amount': 6.7,\n",
       "  'tolls_amount': 0.0,\n",
       "  'improvement_surcharge': 1.0,\n",
       "  'total_amount': 33.6,\n",
       "  'congestion_surcharge': 2.5,\n",
       "  'Airport_fee': 0.0}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ray.data.read_parquet(parquet_file_path)\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "Ray Data 的各类操作都是延迟（Lazy）执行的，即这些操作不是立即执行的，而是遇到数据查看或保存操作时，才会执行，比如：`show()`、`take()`、`iter_rows()`、`write_parquet()` 等操作会触发转换操作。\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `map()` 对这两个字段进行格式化，丢弃日期，只保留24小时制的时间。`map()` 的最重要的参数是一个自定义的函数 `fn`，这个函数对每一条数据进行转换，返回一条。输入数据有 Schema，每条数据是一个 Key-Value 的字典，Key 是 Schema 中的字段名，Value 是对应的数值。\n",
    "\n",
    "下面的例子，我们提取出了每次订单的时长、距离和价格，其他的字段先忽略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 11:50:15,332\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2023-12-15 11:50:15,335\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 200, each read task output is split into 200 smaller blocks.\n",
      "2023-12-15 11:50:15,337\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[Map(transform_row)] -> LimitOperator[limit=1]\n",
      "2023-12-15 11:50:15,338\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-15 11:50:15,339\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad8aa78c1224df0b71422cfc2530174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=251121)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/arrow_block.py:128: FutureWarning: promote has been superseded by mode='default'.\n",
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=251121)\u001b[0m   return transform_pyarrow.concat(tables)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'trip_duration': 1253.0, 'trip_distance': 3.4, 'fare_amount': 21.9}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_row(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    result = {}\n",
    "    result[\"trip_duration\"] = (row[\"tpep_dropoff_datetime\"] - row[\"tpep_pickup_datetime\"]).total_seconds()\n",
    "    result[\"trip_distance\"] = row[\"trip_distance\"]\n",
    "    result[\"fare_amount\"] = row[\"fare_amount\"]\n",
    "    return result\n",
    "\n",
    "row_ds = dataset.map(transform_row)\n",
    "row_ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与 `map()` 有所区别的是，`map_batches()` 是对一个批次进行处理，它模拟的是单机处理时，对整个数据集的操作。其设计思想主要为了方便将之前编写好的、单机的程序，无缝地迁移到 Ray 上：用户先编写一个单机的程序，然后使用 Ray Data 迁移到集群上。`map_batches()` 每个批次的数据格式为 `Dict[str, np.ndarray]` 或 `pd.DataFrame` 或 `pyarrow.Table`，分别对应 NumPy 、pandas 和 Arrow。\n",
    "\n",
    "下面的例子与 `map()` 实现的功能类似，只不过通过 pandas 的形式，对每个 Batch 进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 11:50:22,130\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2023-12-15 11:50:22,139\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 200, each read task output is split into 200 smaller blocks.\n",
      "2023-12-15 11:50:22,143\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[MapBatches(transform_df)] -> LimitOperator[limit=10]\n",
      "2023-12-15 11:50:22,144\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-15 11:50:22,145\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f653ac04ff455999e3d2b84a6cc834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'trip_duration': 1253, 'trip_distance': 3.4, 'fare_amount': 21.9},\n",
       " {'trip_duration': 614, 'trip_distance': 3.4, 'fare_amount': 15.6},\n",
       " {'trip_duration': 1123, 'trip_distance': 10.2, 'fare_amount': 40.8},\n",
       " {'trip_duration': 1406, 'trip_distance': 9.83, 'fare_amount': 39.4},\n",
       " {'trip_duration': 514, 'trip_distance': 1.17, 'fare_amount': 9.3},\n",
       " {'trip_duration': 796, 'trip_distance': 3.6, 'fare_amount': 18.4},\n",
       " {'trip_duration': 1136, 'trip_distance': 3.08, 'fare_amount': 19.8},\n",
       " {'trip_duration': 527, 'trip_distance': 1.1, 'fare_amount': 10.0},\n",
       " {'trip_duration': 237, 'trip_distance': 0.99, 'fare_amount': 6.5},\n",
       " {'trip_duration': 171, 'trip_distance': 0.73, 'fare_amount': 5.1}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_df(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    result_df = pd.DataFrame()\n",
    "    result_df[\"trip_duration\"] = (input_df[\"tpep_dropoff_datetime\"] - input_df[\"tpep_pickup_datetime\"]).dt.seconds\n",
    "    result_df[\"trip_distance\"] = input_df[\"trip_distance\"]\n",
    "    result_df[\"fare_amount\"] = input_df[\"fare_amount\"]\n",
    "    return result_df\n",
    "\n",
    "batch_ds = dataset.map_batches(transform_df, batch_format=\"pandas\")\n",
    "batch_ds.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实现 `map()` 或者 `map_batch()` 时，也可以使用 Python 的 lambda 表达式，即一个匿名的 Python 函数。比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 11:50:27,625\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2023-12-15 11:50:27,626\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 200, each read task output is split into 200 smaller blocks.\n",
      "2023-12-15 11:50:27,627\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[MapBatches(<lambda>)]\n",
      "2023-12-15 11:50:27,628\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-15 11:50:27,628\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d73c49e32640ce9f4efe84e3a2f4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=251175)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/arrow_block.py:128: FutureWarning: promote has been superseded by mode='default'.\n",
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=251175)\u001b[0m   return transform_pyarrow.concat(tables)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=251117)\u001b[0m [FATAL] 2023-12-15 03:50:40.645 event-loop [140229181851456] id=0x2f26480: thread creation failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=251101)\u001b[0m <jemalloc>: arena 0 background thread creation failed (11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过滤后的行数：730352\n"
     ]
    }
   ],
   "source": [
    "filterd_dataset = dataset.map_batches(lambda df: df[df[\"trip_distance\"] > 4], batch_format=\"pandas\")\n",
    "print(f\"过滤后的行数：{filterd_dataset.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 与 Actor\n",
    "\n",
    "可以看到，转换操作本质上是在执行 `fn` ，这个函数接收一个输入，进行转换，得到输出。默认情况下，Ray Data 使用 Task 并行执行转换操作。Ray Task 比较适合无状态的计算，即 `fn` 内不需要被不同数据反复依赖的数据。如果是有状态的计算，需要使用 Ray Actor。比如，加载一个机器学习模型，并用这个模型对所有数据进行预测。下面的例子模拟了机器学习模型预测的过程，模型本身是被反复使用的，所以是有状态的计算。这个例子仅仅作为演示，所使用的并非是训练好的模型，而是一个等价变换 `torch.nn.Identity()`，它将输入原封不动地转换为输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 11:50:42,646\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2023-12-15 11:50:42,647\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 200, each read task output is split into 200 smaller blocks.\n",
      "2023-12-15 11:50:42,648\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[MapBatches(transform_df)] -> LimitOperator[limit=100] -> ActorPoolMapOperator[MapBatches(TorchPredictor)] -> LimitOperator[limit=3]\n",
      "2023-12-15 11:50:42,649\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-15 11:50:42,650\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2023-12-15 11:50:42,708\tINFO actor_pool_map_operator.py:114 -- MapBatches(TorchPredictor): Waiting for 2 pool actors to start...\n",
      "\u001b[33m(raylet)\u001b[0m [2023-12-15 11:50:42,775 C 250981 250981] (raylet) worker_pool.cc:633: Failed to start worker with return value system:11: Resource temporarily unavailable\n",
      "\u001b[33m(raylet)\u001b[0m *** StackTrace Information ***\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0xa9f99a) [0x55d12e3da99a] ray::operator<<()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0xaa1382) [0x55d12e3dc382] ray::SpdLogMessage::Flush()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0xaa1697) [0x55d12e3dc697] ray::RayLog::~RayLog()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x2bc7ea) [0x55d12dbf77ea] ray::raylet::WorkerPool::StartProcess()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x2c2336) [0x55d12dbfd336] ray::raylet::WorkerPool::StartWorkerProcess()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x2c48b9) [0x55d12dbff8b9] ray::raylet::WorkerPool::PopWorker()::{lambda()#1}::operator()()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x2c5d0d) [0x55d12dc00d0d] ray::raylet::WorkerPool::PopWorker()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x32ac6d) [0x55d12dc65c6d] ray::raylet::LocalTaskManager::DispatchScheduledTasksToWorkers()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x32bbd5) [0x55d12dc66bd5] ray::raylet::LocalTaskManager::QueueAndScheduleTask()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x3ea16e) [0x55d12dd2516e] ray::raylet::ClusterTaskManager::ScheduleOnNode()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x3eb550) [0x55d12dd26550] ray::raylet::ClusterTaskManager::ScheduleAndDispatchTasks()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x3e9c73) [0x55d12dd24c73] ray::raylet::ClusterTaskManager::QueueAndScheduleTask()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x27f352) [0x55d12dbba352] ray::raylet::NodeManager::HandleRequestWorkerLease()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x2790fa) [0x55d12dbb40fa] ray::rpc::ServerCallImpl<>::HandleRequestImpl()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x57cfd6) [0x55d12deb7fd6] EventTracker::RecordExecution()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x56ed8e) [0x55d12dea9d8e] std::_Function_handler<>::_M_invoke()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x56f2e6) [0x55d12deaa2e6] boost::asio::detail::completion_handler<>::do_complete()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0xba0d0b) [0x55d12e4dbd0b] boost::asio::detail::scheduler::do_run_one()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0xba3299) [0x55d12e4de299] boost::asio::detail::scheduler::run()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0xba37b2) [0x55d12e4de7b2] boost::asio::io_context::run()\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x18c44a) [0x55d12dac744a] main\n",
      "\u001b[33m(raylet)\u001b[0m /lib64/libc.so.6(__libc_start_main+0xf5) [0x7f8b2dbc7555] __libc_start_main\n",
      "\u001b[33m(raylet)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x1d94c7) [0x55d12db144c7]\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "2023-12-15 11:50:44,473\tWARNING worker.py:2074 -- Raylet is terminated. Termination is unexpected. Possible reasons include: (1) SIGKILL by the user or system OOM killer, (2) Invalid memory access from Raylet causing SIGSEGV or SIGBUS, (3) Other termination signals. Last 20 lines of the Raylet logs:\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x2c2336) [0x55d12dbfd336] ray::raylet::WorkerPool::StartWorkerProcess()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x2c48b9) [0x55d12dbff8b9] ray::raylet::WorkerPool::PopWorker()::{lambda()#1}::operator()()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x2c5d0d) [0x55d12dc00d0d] ray::raylet::WorkerPool::PopWorker()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x32ac6d) [0x55d12dc65c6d] ray::raylet::LocalTaskManager::DispatchScheduledTasksToWorkers()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x32bbd5) [0x55d12dc66bd5] ray::raylet::LocalTaskManager::QueueAndScheduleTask()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x3ea16e) [0x55d12dd2516e] ray::raylet::ClusterTaskManager::ScheduleOnNode()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x3eb550) [0x55d12dd26550] ray::raylet::ClusterTaskManager::ScheduleAndDispatchTasks()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x3e9c73) [0x55d12dd24c73] ray::raylet::ClusterTaskManager::QueueAndScheduleTask()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x27f352) [0x55d12dbba352] ray::raylet::NodeManager::HandleRequestWorkerLease()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x2790fa) [0x55d12dbb40fa] ray::rpc::ServerCallImpl<>::HandleRequestImpl()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x57cfd6) [0x55d12deb7fd6] EventTracker::RecordExecution()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x56ed8e) [0x55d12dea9d8e] std::_Function_handler<>::_M_invoke()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x56f2e6) [0x55d12deaa2e6] boost::asio::detail::completion_handler<>::do_complete()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0xba0d0b) [0x55d12e4dbd0b] boost::asio::detail::scheduler::do_run_one()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0xba3299) [0x55d12e4de299] boost::asio::detail::scheduler::run()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0xba37b2) [0x55d12e4de7b2] boost::asio::io_context::run()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x18c44a) [0x55d12dac744a] main\n",
      "    /lib64/libc.so.6(__libc_start_main+0xf5) [0x7f8b2dbc7555] __libc_start_main\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x1d94c7) [0x55d12db144c7]\n",
      "    \n",
      "\n",
      "2023-12-15 11:50:44,662\tWARNING worker.py:2074 -- Raylet is terminated. Termination is unexpected. Possible reasons include: (1) SIGKILL by the user or system OOM killer, (2) Invalid memory access from Raylet causing SIGSEGV or SIGBUS, (3) Other termination signals. Last 20 lines of the Raylet logs:\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x2c2336) [0x55d12dbfd336] ray::raylet::WorkerPool::StartWorkerProcess()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x2c48b9) [0x55d12dbff8b9] ray::raylet::WorkerPool::PopWorker()::{lambda()#1}::operator()()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x2c5d0d) [0x55d12dc00d0d] ray::raylet::WorkerPool::PopWorker()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x32ac6d) [0x55d12dc65c6d] ray::raylet::LocalTaskManager::DispatchScheduledTasksToWorkers()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x32bbd5) [0x55d12dc66bd5] ray::raylet::LocalTaskManager::QueueAndScheduleTask()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x3ea16e) [0x55d12dd2516e] ray::raylet::ClusterTaskManager::ScheduleOnNode()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x3eb550) [0x55d12dd26550] ray::raylet::ClusterTaskManager::ScheduleAndDispatchTasks()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x3e9c73) [0x55d12dd24c73] ray::raylet::ClusterTaskManager::QueueAndScheduleTask()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x27f352) [0x55d12dbba352] ray::raylet::NodeManager::HandleRequestWorkerLease()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x2790fa) [0x55d12dbb40fa] ray::rpc::ServerCallImpl<>::HandleRequestImpl()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x57cfd6) [0x55d12deb7fd6] EventTracker::RecordExecution()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x56ed8e) [0x55d12dea9d8e] std::_Function_handler<>::_M_invoke()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x56f2e6) [0x55d12deaa2e6] boost::asio::detail::completion_handler<>::do_complete()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0xba0d0b) [0x55d12e4dbd0b] boost::asio::detail::scheduler::do_run_one()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0xba3299) [0x55d12e4de299] boost::asio::detail::scheduler::run()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0xba37b2) [0x55d12e4de7b2] boost::asio::io_context::run()\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x18c44a) [0x55d12dac744a] main\n",
      "    /lib64/libc.so.6(__libc_start_main+0xf5) [0x7f8b2dbc7555] __libc_start_main\n",
      "    /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet(+0x1d94c7) [0x55d12db144c7]\n",
      "    \n",
      "\n",
      "2023-12-15 11:50:57,286\tWARNING worker.py:2074 -- The node with node id: f05f4379415242bd78d13449ec6aa43f493524ef9cce504d0ea5c2b6 and address: 10.10.252.130 and node name: 10.10.252.130 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a \t(1) raylet crashes unexpectedly (OOM, preempted node, etc.) \n",
      "\t(2) raylet has lagging heartbeats due to slow network or busy workload.\n",
      "[2023-12-15 11:50:57,377 E 250690 251098] core_worker.cc:592: :info_message: Attempting to recover 211 lost objects by resubmitting their tasks. To disable object reconstruction, set @ray.remote(max_retries=0).\n"
     ]
    },
    {
     "ename": "RayActorError",
     "evalue": "The actor died unexpectedly before finishing this task.\n\tclass_name: _MapWorker\n\tactor_id: a670a0be9412d1a1e2150a0001000000\n\tnamespace: 725e0160-86c4-4279-b59e-047d46e7baba\nThe actor is dead because its owner has died. Owner Id: 01000000ffffffffffffffffffffffffffffffffffffffffffffffff Owner Ip address: 10.10.252.130 Owner worker exit type: SYSTEM_ERROR Worker exit detail: Owner's node has crashed.\nThe actor never ran - it was cancelled before it started running.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pred\n\u001b[1;32m     14\u001b[0m pred_ds \u001b[38;5;241m=\u001b[39m batch_ds\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mmap_batches(TorchPredictor, compute\u001b[38;5;241m=\u001b[39mray\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mActorPoolStrategy(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m---> 15\u001b[0m \u001b[43mpred_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dispy/lib/python3.11/site-packages/ray/data/dataset.py:2390\u001b[0m, in \u001b[0;36mDataset.take\u001b[0;34m(self, limit)\u001b[0m\n\u001b[1;32m   2387\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2389\u001b[0m limited_ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlimit(limit)\n\u001b[0;32m-> 2390\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlimited_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2391\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2392\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dispy/lib/python3.11/site-packages/ray/data/iterator.py:219\u001b[0m, in \u001b[0;36mDataIterator.iter_rows.<locals>._wrapped_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_iterator\u001b[39m():\n\u001b[0;32m--> 219\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBlockAccessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBlockAccessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_to_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_row_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dispy/lib/python3.11/site-packages/ray/data/iterator.py:164\u001b[0m, in \u001b[0;36mDataIterator.iter_batches.<locals>._create_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Iterate through the dataset from the start each time\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# _iterator_gen is called.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# This allows multiple iterations of the dataset without\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# needing to explicitly call `iter_batches()` multiple times.\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m block_iterator, stats, blocks_owned_by_consumer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_block_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m    167\u001b[0m     iter_batches(\n\u001b[1;32m    168\u001b[0m         block_iterator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m     )\n\u001b[1;32m    180\u001b[0m )\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iterator:\n",
      "File \u001b[0;32m~/.conda/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/iterator/iterator_impl.py:32\u001b[0m, in \u001b[0;36mDataIteratorImpl._to_block_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_block_iterator\u001b[39m(\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     26\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m     30\u001b[0m ]:\n\u001b[1;32m     31\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_dataset\n\u001b[0;32m---> 32\u001b[0m     block_iterator, stats, executor \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_to_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     ds\u001b[38;5;241m.\u001b[39m_current_executor \u001b[38;5;241m=\u001b[39m executor\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m block_iterator, stats, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/plan.py:548\u001b[0m, in \u001b[0;36mExecutionPlan.execute_to_iterator\u001b[0;34m(self, allow_clear_input_blocks, force_read)\u001b[0m\n\u001b[1;32m    546\u001b[0m gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(block_iter)\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 548\u001b[0m     block_iter \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain([\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m], gen)\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/execution/legacy_compat.py:51\u001b[0m, in \u001b[0;36mexecute_to_legacy_block_iterator\u001b[0;34m(executor, plan, allow_clear_input_blocks, dataset_uuid)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_to_legacy_block_iterator\u001b[39m(\n\u001b[1;32m     45\u001b[0m     executor: Executor,\n\u001b[1;32m     46\u001b[0m     plan: ExecutionPlan,\n\u001b[1;32m     47\u001b[0m     allow_clear_input_blocks: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m     48\u001b[0m     dataset_uuid: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     49\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Tuple[ObjectRef[Block], BlockMetadata]]:\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Same as execute_to_legacy_bundle_iterator but returning blocks and metadata.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     bundle_iter \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_to_legacy_bundle_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_input_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_uuid\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m bundle \u001b[38;5;129;01min\u001b[39;00m bundle_iter:\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m block, metadata \u001b[38;5;129;01min\u001b[39;00m bundle\u001b[38;5;241m.\u001b[39mblocks:\n",
      "File \u001b[0;32m~/.conda/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/execution/legacy_compat.py:89\u001b[0m, in \u001b[0;36mexecute_to_legacy_bundle_iterator\u001b[0;34m(executor, plan, allow_clear_input_blocks, dataset_uuid, dag_rewrite)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dag_rewrite:\n\u001b[1;32m     87\u001b[0m     dag \u001b[38;5;241m=\u001b[39m dag_rewrite(dag)\n\u001b[0;32m---> 89\u001b[0m bundle_iter \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bundle_iter\n",
      "File \u001b[0;32m~/.conda/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/execution/streaming_executor.py:114\u001b[0m, in \u001b[0;36mStreamingExecutor.execute\u001b[0;34m(self, dag, initial_stats)\u001b[0m\n\u001b[1;32m    107\u001b[0m         logger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTip: For detailed progress reporting, run \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`ray.data.DataContext.get_current().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution_options.verbose_progress = True`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m         )\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Setup the streaming DAG topology and start the runner thread.\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_topology, _ \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_streaming_topology\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backpressure_policies \u001b[38;5;241m=\u001b[39m get_backpressure_policies(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_topology)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dag, InputDataBuffer):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# Note: DAG must be initialized in order to query num_outputs_total.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/execution/streaming_executor_state.py:318\u001b[0m, in \u001b[0;36mbuild_streaming_topology\u001b[0;34m(dag, options)\u001b[0m\n\u001b[1;32m    315\u001b[0m     op\u001b[38;5;241m.\u001b[39mstart(options)\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op_state\n\u001b[0;32m--> 318\u001b[0m \u001b[43msetup_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Create the progress bars starting from the first operator to run.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# Note that the topology dict is in topological sort order. Index zero is reserved\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# for global progress information.\u001b[39;00m\n\u001b[1;32m    323\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/execution/streaming_executor_state.py:309\u001b[0m, in \u001b[0;36mbuild_streaming_topology.<locals>.setup_state\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    307\u001b[0m inqueues \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, parent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(op\u001b[38;5;241m.\u001b[39minput_dependencies):\n\u001b[0;32m--> 309\u001b[0m     parent_state \u001b[38;5;241m=\u001b[39m \u001b[43msetup_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m     inqueues\u001b[38;5;241m.\u001b[39mappend(parent_state\u001b[38;5;241m.\u001b[39moutqueue)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# Create state.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/execution/streaming_executor_state.py:315\u001b[0m, in \u001b[0;36mbuild_streaming_topology.<locals>.setup_state\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    313\u001b[0m op_state \u001b[38;5;241m=\u001b[39m OpState(op, inqueues)\n\u001b[1;32m    314\u001b[0m topology[op] \u001b[38;5;241m=\u001b[39m op_state\n\u001b[0;32m--> 315\u001b[0m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op_state\n",
      "File \u001b[0;32m~/.conda/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/execution/operators/actor_pool_map_operator.py:117\u001b[0m, in \u001b[0;36mActorPoolMapOperator.start\u001b[0;34m(self, options)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# We synchronously wait for the initial number of actors to start. This avoids\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# situations where the scheduler is unable to schedule downstream operators\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# due to lack of available actors, causing an initial \"pileup\" of objects on\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# upstream operators, leading to a spike in memory usage prior to steady state.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m logger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Waiting for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(refs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pool actors to start...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m )\n\u001b[0;32m--> 117\u001b[0m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEFAULT_WAIT_FOR_MIN_ACTORS_SEC\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dispy/lib/python3.11/site-packages/ray/_private/auto_init_hook.py:24\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     23\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dispy/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dispy/lib/python3.11/site-packages/ray/_private/worker.py:2565\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2563\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   2564\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2565\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m   2567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_individual_id:\n\u001b[1;32m   2568\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died unexpectedly before finishing this task.\n\tclass_name: _MapWorker\n\tactor_id: a670a0be9412d1a1e2150a0001000000\n\tnamespace: 725e0160-86c4-4279-b59e-047d46e7baba\nThe actor is dead because its owner has died. Owner Id: 01000000ffffffffffffffffffffffffffffffffffffffffffffffff Owner Ip address: 10.10.252.130 Owner worker exit type: SYSTEM_ERROR Worker exit detail: Owner's node has crashed.\nThe actor never ran - it was cancelled before it started running."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m [2023-12-15 11:52:16,023 C 253563 253563] core_worker.cc:2664:  Check failed: _s.ok() Bad status: IOError: Broken pipe\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m *** StackTrace Information ***\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(+0xf1733a) [0x7f6cf696d33a] ray::operator<<()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(+0xf18e22) [0x7f6cf696ee22] ray::SpdLogMessage::Flush()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(_ZN3ray6RayLogD1Ev+0x37) [0x7f6cf696f137] ray::RayLog::~RayLog()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(_ZN3ray4core10CoreWorker11ExecuteTaskERKNS_17TaskSpecificationERKSt10shared_ptrISt13unordered_mapISsSt6vectorISt4pairIldESaIS9_EESt4hashISsESt8equal_toISsESaIS8_IKSsSB_EEEEPS7_IS8_INS_8ObjectIDES5_INS_9RayObjectEEESaISQ_EEST_PS7_IS8_ISN_bESaISU_EEPN6google8protobuf16RepeatedPtrFieldINS_3rpc20ObjectReferenceCountEEEPbPSs+0x6b6) [0x7f6cf61acc66] ray::core::CoreWorker::ExecuteTask()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(_ZNSt17_Function_handlerIFN3ray6StatusERKNS0_17TaskSpecificationESt10shared_ptrISt13unordered_mapISsSt6vectorISt4pairIldESaIS9_EESt4hashISsESt8equal_toISsESaIS8_IKSsSB_EEEEPS7_IS8_INS0_8ObjectIDES5_INS0_9RayObjectEEESaISO_EESR_PS7_IS8_ISL_bESaISS_EEPN6google8protobuf16RepeatedPtrFieldINS0_3rpc20ObjectReferenceCountEEEPbPSsESt5_BindIFMNS0_4core10CoreWorkerEFS1_S4_RKSK_SR_SR_SV_S12_S13_S14_EPS18_St12_PlaceholderILi1EES1E_ILi2EES1E_ILi3EES1E_ILi4EES1E_ILi5EES1E_ILi6EES1E_ILi7EES1E_ILi8EEEEE9_M_invokeERKSt9_Any_dataS4_OSK_OSR_S1U_OSV_OS12_OS13_OS14_+0x58) [0x7f6cf60e08e8] std::_Function_handler<>::_M_invoke()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(+0x77f454) [0x7f6cf61d5454] ray::core::CoreWorkerDirectTaskReceiver::HandleTask()::{lambda()#1}::operator()()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(+0x78075a) [0x7f6cf61d675a] std::_Function_handler<>::_M_invoke()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(+0x797e1e) [0x7f6cf61ede1e] ray::core::InboundRequest::Accept()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(_ZN3ray4core20ActorSchedulingQueue31AcceptRequestOrRejectIfCanceledENS_6TaskIDERNS0_14InboundRequestE+0x10c) [0x7f6cf61ef12c] ray::core::ActorSchedulingQueue::AcceptRequestOrRejectIfCanceled()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(+0x79c09b) [0x7f6cf61f209b] ray::core::ActorSchedulingQueue::ScheduleRequests()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(_ZN3ray4core20ActorSchedulingQueue3AddEllSt8functionIFvS2_IFvNS_6StatusES2_IFvvEES5_EEEES2_IFvRKS3_S7_EES7_RKSsRKSt10shared_ptrINS_27FunctionDescriptorInterfaceEENS_6TaskIDERKSt6vectorINS_3rpc15ObjectReferenceESaISO_EE+0x400) [0x7f6cf61f3b70] ray::core::ActorSchedulingQueue::Add()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(_ZN3ray4core28CoreWorkerDirectTaskReceiver10HandleTaskERKNS_3rpc15PushTaskRequestEPNS2_13PushTaskReplyESt8functionIFvNS_6StatusES8_IFvvEESB_EE+0x1216) [0x7f6cf61d4de6] ray::core::CoreWorkerDirectTaskReceiver::HandleTask()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(+0x7212d5) [0x7f6cf61772d5] std::_Function_handler<>::_M_invoke()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(+0xa0ae86) [0x7f6cf6460e86] EventTracker::RecordExecution()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(+0x9fcb5e) [0x7f6cf6452b5e] std::_Function_handler<>::_M_invoke()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(+0x9fd0b6) [0x7f6cf64530b6] boost::asio::detail::completion_handler<>::do_complete()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(+0x1020adb) [0x7f6cf6a76adb] boost::asio::detail::scheduler::do_run_one()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(+0x1022ab9) [0x7f6cf6a78ab9] boost::asio::detail::scheduler::run()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(+0x10231c2) [0x7f6cf6a791c2] boost::asio::io_context::run()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(_ZN3ray4core10CoreWorker20RunTaskExecutionLoopEv+0xcd) [0x7f6cf6175acd] ray::core::CoreWorker::RunTaskExecutionLoop()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(_ZN3ray4core21CoreWorkerProcessImpl26RunWorkerTaskExecutionLoopEv+0x8c) [0x7f6cf61b82ac] ray::core::CoreWorkerProcessImpl::RunWorkerTaskExecutionLoop()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(_ZN3ray4core17CoreWorkerProcess20RunTaskExecutionLoopEv+0x1d) [0x7f6cf61b845d] ray::core::CoreWorkerProcess::RunTaskExecutionLoop()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/_raylet.so(+0x57b6a7) [0x7f6cf5fd16a7] __pyx_pw_3ray_7_raylet_10CoreWorker_7run_task_loop()\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m ray::AutoscalingRequester() [0x535490] method_vectorcall_NOARGS\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m ray::AutoscalingRequester(PyObject_Vectorcall+0x31) [0x51bff1] PyObject_Vectorcall\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m ray::AutoscalingRequester(_PyEval_EvalFrameDefault+0x755) [0x50f025] _PyEval_EvalFrameDefault\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m ray::AutoscalingRequester() [0x5c82ce] _PyEval_Vector\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m ray::AutoscalingRequester(PyEval_EvalCode+0x9f) [0x5c79cf] PyEval_EvalCode\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m ray::AutoscalingRequester() [0x5e8807] run_eval_code_obj\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m ray::AutoscalingRequester() [0x5e4e40] run_mod\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m ray::AutoscalingRequester() [0x5f9132] pyrun_file\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m ray::AutoscalingRequester(_PyRun_SimpleFileObject+0x19f) [0x5f871f] _PyRun_SimpleFileObject\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m ray::AutoscalingRequester(_PyRun_AnyFileObject+0x43) [0x5f8473] _PyRun_AnyFileObject\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m ray::AutoscalingRequester(Py_RunMain+0x2ee) [0x5f2fee] Py_RunMain\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m ray::AutoscalingRequester(Py_BytesMain+0x39) [0x5b6e19] Py_BytesMain\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m /lib64/libc.so.6(__libc_start_main+0xf5) [0x7f6cfd944555] __libc_start_main\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m ray::AutoscalingRequester() [0x5b6c6f]\n",
      "\u001b[36m(AutoscalingRequester pid=253563)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "class TorchPredictor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = torch.nn.Identity()\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
    "        pred = {}\n",
    "        inputs = torch.as_tensor(df['trip_distance'], dtype=torch.float32)\n",
    "        with torch.inference_mode():\n",
    "            pred[\"output\"] = self.model(inputs).detach().numpy()\n",
    "        return pred\n",
    "\n",
    "pred_ds = batch_ds.limit(100).map_batches(TorchPredictor, compute=ray.data.ActorPoolStrategy(size=2))\n",
    "pred_ds.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 Actor 大概分为 3 步骤：\n",
    "\n",
    "1. 创建一个类，这个类包含一个 `__init__()` 方法和一个 `__call__()` 方法。`__init__()` 方法初始化一些可被反复使用的状态数据，`__call__()` 方法实现转换操作。可以参考刚才实现的 `TorchPredictor` 类。\n",
    "2. 创建一个 `ActorPoolStrategy`，指定一共多少个 Worker。\n",
    "3. 调用 `map_batch()` 方法，将 `ActorPoolStrategy` 传递给 `compute` 参数。\n",
    "\n",
    "## 分组\n",
    "\n",
    "数据处理中另外一个经常使用的原语是分组聚合，Ray Data 提供了： [groupby()](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.groupby.html)。Ray Data 先调用 `groupby()`，对数据按照某些字段进行分组，再调用 `map_groups()` 对分组之后的数据进行聚合。\n",
    "\n",
    "`groupby(key)` 的参数 `key` 是需要进行分组的字段；`map_groups(fn)` 的参数 `fn`，对同一个组的数据进行操作。Ray Data 预置了一些聚合函数，比如常见的求和 `sum()`，最大值 `max()`，平均值 `mean()` 等。比如下面的例子使用 `mean()` 对 `value` 字段进行聚合。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 11:17:28,930\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=20]\n",
      "2023-12-15 11:17:28,932\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-15 11:17:28,934\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033864b14a0343f38739ade698ea4f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Aggregate 1:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25723616a27410985736b4db800ac7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 2:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f48e54f701745ccbf72f474f90095ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 3:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14d01285a11480fa1353fb686f075fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412181d9acca4010babf05d9e4ee8260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 0:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dispy/.conda/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/arrow_block.py:128: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return transform_pyarrow.concat(tables)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group': 1, 'mean(value)': 1.5}\n",
      "{'group': 2, 'mean(value)': 3.5}\n"
     ]
    }
   ],
   "source": [
    "ds = ray.data.from_items([\n",
    "    {\"group\": 1, \"value\": 1},\n",
    "    {\"group\": 1, \"value\": 2},\n",
    "    {\"group\": 2, \"value\": 3},\n",
    "    {\"group\": 2, \"value\": 4}])\n",
    "mean_ds = ds.groupby(\"group\").mean(\"value\")\n",
    "mean_ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dispy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

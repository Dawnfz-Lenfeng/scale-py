{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ray-data-transform)=\n",
    "# 数据转换\n",
    "\n",
    "数据处理的核心在于对数据进行一系列的转换（Transform），本节将介绍：\n",
    "\n",
    "* 如何对一行、一批次进行转换\n",
    "* 如何进行分组 `groupby`\n",
    "* 如何随机洗牌\n",
    "\n",
    "## 转换\n",
    "\n",
    "### map() 与 map_batches()\n",
    "\n",
    "Ray Data 提供了两类数据转换操作，如 {numref}`map-map-batches` 所示：\n",
    "\n",
    "* 每行数据，可以用 `Dataset.map()` 和 `Dataset.flat_map()` 这两个 API，即对每一行数据一一进行转换。这与其他大数据框架（Spark 或者 Flink）类似。输入一行，输出一行。\n",
    "* 将多行数据打包为一个批次（Batch），对一个批次的数据进行转换：[`Dataset.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html)。输入一个 Batch，输出一个 Batch。\n",
    "\n",
    "```{figure} ../img/ch-ray-air/map-map-batches.svg\n",
    "---\n",
    "width: 800px\n",
    "name: map-map-batches\n",
    "---\n",
    "map() v.s. map_batches()\n",
    "```\n",
    "\n",
    "我们仍以纽约出租车数据为例，演示如何使用这两类转换操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-14 13:40:10,994\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-12-14 13:40:16,312\tINFO worker.py:1673 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件夹 /Users/luweizheng/Projects/py-101/distributed-python/ch-ray-data/../data/nyc-taxi 已存在，无需操作。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import urllib.request\n",
    "\n",
    "import ray\n",
    "\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "\n",
    "ray.init()\n",
    "\n",
    "folder_path = os.path.join(os.getcwd(), \"../data/nyc-taxi\")\n",
    "download_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-06.parquet\"\n",
    "file_name = download_url.split(\"/\")[-1]\n",
    "parquet_file_path = os.path.join(folder_path, file_name)\n",
    "if not os.path.exists(folder_path):\n",
    "    # 创建文件夹\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"文件夹 {folder_path} 不存在，已创建。\")\n",
    "    # 下载并保存 Parquet 文件\n",
    "    with urllib.request.urlopen(download_url) as response, open(parquet_file_path, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)\n",
    "    print(\"数据已下载并保存为 Parquet 文件。\")\n",
    "else:\n",
    "    print(f\"文件夹 {folder_path} 已存在，无需操作。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据到 `Dataset` 类，先查看原有的数据格式，其中 `tpep_pickup_datetime` 和 `tpep_dropoff_datetime` 分别为乘客上车和下车时间，包含了日期和时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 13:40:18,847\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "(pid=64922) Parquet Files Sample 0:   0%|          | 0/1 [00:00<?, ?it/s]2023-12-14 13:40:21,041\tINFO dataset.py:2383 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2023-12-14 13:40:21,048\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2023-12-14 13:40:21,049\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 200, each read task output is split into 200 smaller blocks.\n",
      "2023-12-14 13:40:21,050\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> LimitOperator[limit=1]\n",
      "2023-12-14 13:40:21,052\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-14 13:40:21,053\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "                                                                                 \n",
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=64922)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/arrow_block.py:128: FutureWarning: promote has been superseded by mode='default'.\n",
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=64922)\u001b[0m   return transform_pyarrow.concat(tables)                        \n",
      "                                                                                                                          \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'VendorID': 1,\n",
       "  'tpep_pickup_datetime': datetime.datetime(2023, 6, 1, 0, 8, 48),\n",
       "  'tpep_dropoff_datetime': datetime.datetime(2023, 6, 1, 0, 29, 41),\n",
       "  'passenger_count': 1,\n",
       "  'trip_distance': 3.4,\n",
       "  'RatecodeID': 1,\n",
       "  'store_and_fwd_flag': 'N',\n",
       "  'PULocationID': 140,\n",
       "  'DOLocationID': 238,\n",
       "  'payment_type': 1,\n",
       "  'fare_amount': 21.9,\n",
       "  'extra': 3.5,\n",
       "  'mta_tax': 0.5,\n",
       "  'tip_amount': 6.7,\n",
       "  'tolls_amount': 0.0,\n",
       "  'improvement_surcharge': 1.0,\n",
       "  'total_amount': 33.6,\n",
       "  'congestion_surcharge': 2.5,\n",
       "  'Airport_fee': 0.0}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ray.data.read_parquet(parquet_file_path)\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `map()` 对这两个字段进行格式化，丢弃日期，只保留24小时制的时间。`map()` 的最重要的参数是一个自定义的函数 `fn`，这个函数对每一行数据进行转换，返回一行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 13:40:22,158\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2023-12-14 13:40:22,159\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 200, each read task output is split into 200 smaller blocks.\n",
      "2023-12-14 13:40:22,161\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[Map(format_datetime)] -> LimitOperator[limit=1]\n",
      "2023-12-14 13:40:22,165\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-14 13:40:22,166\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "Running: 0.0/8.0 CPU, 0.0/0.0 GPU, 0.0 MiB/512.0 MiB object_store_memory:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                          \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'VendorID': 1,\n",
       "  'tpep_pickup_datetime': '0008',\n",
       "  'tpep_dropoff_datetime': '0029',\n",
       "  'passenger_count': 1,\n",
       "  'trip_distance': 3.4,\n",
       "  'RatecodeID': 1,\n",
       "  'store_and_fwd_flag': 'N',\n",
       "  'PULocationID': 140,\n",
       "  'DOLocationID': 238,\n",
       "  'payment_type': 1,\n",
       "  'fare_amount': 21.9,\n",
       "  'extra': 3.5,\n",
       "  'mta_tax': 0.5,\n",
       "  'tip_amount': 6.7,\n",
       "  'tolls_amount': 0.0,\n",
       "  'improvement_surcharge': 1.0,\n",
       "  'total_amount': 33.6,\n",
       "  'congestion_surcharge': 2.5,\n",
       "  'Airport_fee': 0.0}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_datetime(row):\n",
    "    row['tpep_pickup_datetime'] = row['tpep_pickup_datetime'].strftime(\"%H%M\")\n",
    "    row['tpep_dropoff_datetime'] = row['tpep_dropoff_datetime'].strftime(\"%H%M\")\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(format_datetime)\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与 `map()` 有所区别的是，`map_batches()` 是对一个批次进行处理，它模拟的是单机处理时，对整个数据集的操作。其设计思想主要为了方便将之前编写好的、单机的程序，无缝地迁移到 Ray 上：用户先编写一个单机的程序，然后使用 Ray Data 迁移到集群上。在 `map_batches()` 上，每个批次的数据格式为 `Dict[str, np.ndarray]` 或 `pd.DataFrame` 或 `pyarrow.Table` 表示，分别对应使用 NumPy 、pandas 和 Arrow 时，进行单机处理的业务逻辑。\n",
    "\n",
    "下面的例子过滤某个字段的值，可以看到，经过过滤之后，数据的条数大大减少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 13:40:32,182\tWARNING plan.py:577 -- Warning: The Ray cluster currently does not have any available CPUs. The Dataset job will hang unless more CPUs are freed up. A common reason is that cluster resources are used by Actors or Tune trials; see the following link for more details: https://docs.ray.io/en/latest/data/data-internals.html#ray-data-and-tune\n",
      "2023-12-14 13:40:32,191\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2023-12-14 13:40:32,193\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 200, each read task output is split into 200 smaller blocks.\n",
      "2023-12-14 13:40:32,196\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[Map(format_datetime)] -> LimitOperator[limit=200]\n",
      "2023-12-14 13:40:32,198\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-14 13:40:32,199\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=64923)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/arrow_block.py:128: FutureWarning: promote has been superseded by mode='default'.\n",
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=64923)\u001b[0m   return transform_pyarrow.concat(tables)                        \n",
      "2023-12-14 13:40:41,752\tINFO split_read_output_blocks.py:101 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2023-12-14 13:40:41,753\tINFO split_read_output_blocks.py:106 -- To satisfy the requested parallelism of 200, each read task output is split into 200 smaller blocks.\n",
      "2023-12-14 13:40:41,754\tINFO streaming_executor.py:104 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[Map(format_datetime)] -> LimitOperator[limit=200] -> TaskPoolMapOperator[MapBatches(<lambda>)]\n",
      "2023-12-14 13:40:41,754\tINFO streaming_executor.py:105 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-12-14 13:40:41,755\tINFO streaming_executor.py:107 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集中的行数：200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=64926)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/arrow_block.py:128: FutureWarning: promote has been superseded by mode='default'.\n",
      "\u001b[36m(ReadParquet->SplitBlocks(200) pid=64926)\u001b[0m   return transform_pyarrow.concat(tables)                        \n",
      "                                                                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过滤后的行数：4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dataset = dataset.limit(200)\n",
    "print(f\"数据集中的行数：{dataset.count()}\")\n",
    "print()\n",
    "lambda_filterd_dataset = dataset.map_batches(lambda df: df[df[\"passenger_count\"] == 0],  batch_format=\"pandas\")\n",
    "print(f\"过滤后的行数：{lambda_filterd_dataset.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 与 Actor\n",
    "\n",
    "可以看到，转换操作本质上是在执行 `fn` ，这个函数接收一个输入，进行转换，得到输出。默认情况下，Ray Data 使用 Task 并行执行转换操作。Ray Task 比较适合无状态的计算，即 `fn` 内不需要被不同数据反复依赖的数据。如果是有状态的计算，需要使用 Ray Actor。比如，加载一个机器学习模型，并用这个模型对所有数据进行预测。下面的例子模拟了机器学习模型预测的过程，模型本身是被反复使用的，所以是有状态的计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/air/util/tensor_extensions/arrow.py:76: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   super().__init__(pa.list_(dtype))\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/arrow_serialization.py:145: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m \n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   for column_name in t.column_names:\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/arrow_serialization.py:145: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   for column_name in t.column_names:\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/arrow_serialization.py:146: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m \n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   column = t[column_name]\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/arrow_serialization.py:146: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   column = t[column_name]\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/arrow_serialization.py:198: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m \n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   for chunk in ca.chunks:\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/arrow_serialization.py:198: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   for chunk in ca.chunks:\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/arrow_serialization.py:151: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m \n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   if not _is_dense_union(column.type) and _is_in_test():\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/arrow_serialization.py:151: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   if not _is_dense_union(column.type) and _is_in_test():\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/arrow_serialization.py:156: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m \n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   if type(column.type) not in _serialization_fallback_set:\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/arrow_serialization.py:156: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   if type(column.type) not in _serialization_fallback_set:\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/arrow_serialization.py:159: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m \n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   f\"serialization of column '{column_name}' of type {column.type} \"\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/arrow_serialization.py:159: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   f\"serialization of column '{column_name}' of type {column.type} \"\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m Failed to complete optimized serialization of Arrow Table, serialization of column 'data' of type extension<arrow.py_extension_type<pyarrow.lib.UnknownExtensionType>> failed, so we're falling back to Arrow IPC serialization for the table. Note that this may result in slower serialization and more worker memory utilization. Serialization error:\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   File \"/Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/arrow_serialization.py\", line 149, in _arrow_table_reduce\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m     reduced_column = _arrow_chunked_array_reduce(column)\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   File \"/Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/arrow_serialization.py\", line 199, in _arrow_chunked_array_reduce\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m     chunk_payload = PicklableArrayPayload.from_array(chunk)\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   File \"/Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/arrow_serialization.py\", line 245, in from_array\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m     return _array_to_array_payload(a)\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   File \"/Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/arrow_serialization.py\", line 339, in _array_to_array_payload\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m     raise ValueError(\"Unhandled Arrow array type:\", a.type)\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m ValueError: ('Unhandled Arrow array type:', UnknownExtensionType(ListType(list<item: double>)))\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/arrow_serialization.py:165: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m \n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   _serialization_fallback_set.add(type(column.type))\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/arrow_serialization.py:165: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   _serialization_fallback_set.add(type(column.type))\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/cloudpickle/cloudpickle_fast.py:733: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m \n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   return Pickler.dump(self, obj)\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m /Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/cloudpickle/cloudpickle_fast.py:733: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[36m(ndarray_to_block pid=64926)\u001b[0m   return Pickler.dump(self, obj)\n"
     ]
    },
    {
     "ename": "RayTaskError(NotImplementedError)",
     "evalue": "\u001b[36mray::ndarray_to_block()\u001b[39m (pid=64926, ip=127.0.0.1)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n             ^^^^^^^^^^^^^\n  File \"/Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 88, in dumps\n    cp.dump(obj)\n  File \"/Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 733, in dump\n    return Pickler.dump(self, obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/types.pxi\", line 1710, in pyarrow.lib.PyExtensionType.__reduce__\nNotImplementedError: Please implement UnknownExtensionType.__reduce__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(NotImplementedError)\u001b[0m         Traceback (most recent call last)",
      "\u001b[1;32m/Users/luweizheng/Projects/py-101/distributed-python/ch-ray-data/ray-data-transform.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luweizheng/Projects/py-101/distributed-python/ch-ray-data/ray-data-transform.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m             batch[\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(inputs)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luweizheng/Projects/py-101/distributed-python/ch-ray-data/ray-data-transform.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m batch\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luweizheng/Projects/py-101/distributed-python/ch-ray-data/ray-data-transform.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m ds \u001b[39m=\u001b[39m (\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/luweizheng/Projects/py-101/distributed-python/ch-ray-data/ray-data-transform.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     ray\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfrom_numpy(np\u001b[39m.\u001b[39;49mones((\u001b[39m32\u001b[39;49m, \u001b[39m100\u001b[39;49m)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luweizheng/Projects/py-101/distributed-python/ch-ray-data/ray-data-transform.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m.\u001b[39mmap_batches(TorchPredictor, compute\u001b[39m=\u001b[39mray\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mActorPoolStrategy(size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luweizheng/Projects/py-101/distributed-python/ch-ray-data/ray-data-transform.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/data/read_api.py:2141\u001b[0m, in \u001b[0;36mfrom_numpy\u001b[0;34m(ndarrays)\u001b[0m\n\u001b[1;32m   2138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ndarrays, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m   2139\u001b[0m     ndarrays \u001b[39m=\u001b[39m [ndarrays]\n\u001b[0;32m-> 2141\u001b[0m \u001b[39mreturn\u001b[39;00m from_numpy_refs([ray\u001b[39m.\u001b[39;49mput(ndarray) \u001b[39mfor\u001b[39;49;00m ndarray \u001b[39min\u001b[39;49;00m ndarrays])\n",
      "File \u001b[0;32m~/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/data/read_api.py:2189\u001b[0m, in \u001b[0;36mfrom_numpy_refs\u001b[0;34m(ndarrays)\u001b[0m\n\u001b[1;32m   2187\u001b[0m res \u001b[39m=\u001b[39m [ndarray_to_block_remote\u001b[39m.\u001b[39mremote(ndarray, ctx) \u001b[39mfor\u001b[39;00m ndarray \u001b[39min\u001b[39;00m ndarrays]\n\u001b[1;32m   2188\u001b[0m blocks, metadata \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlist\u001b[39m, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mres))\n\u001b[0;32m-> 2189\u001b[0m metadata \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mget(metadata)\n\u001b[1;32m   2191\u001b[0m logical_plan \u001b[39m=\u001b[39m LogicalPlan(FromNumpy(blocks, metadata))\n\u001b[1;32m   2193\u001b[0m \u001b[39mreturn\u001b[39;00m MaterializedDataset(\n\u001b[1;32m   2194\u001b[0m     ExecutionPlan(\n\u001b[1;32m   2195\u001b[0m         BlockList(blocks, metadata, owned_by_consumer\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2199\u001b[0m     logical_plan,\n\u001b[1;32m   2200\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/auto_init_hook.py:24\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mauto_init_wrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     23\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/_private/worker.py:2563\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2561\u001b[0m     worker\u001b[39m.\u001b[39mcore_worker\u001b[39m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m   2562\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m-> 2563\u001b[0m     \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   2564\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2565\u001b[0m     \u001b[39mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(NotImplementedError)\u001b[0m: \u001b[36mray::ndarray_to_block()\u001b[39m (pid=64926, ip=127.0.0.1)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n             ^^^^^^^^^^^^^\n  File \"/Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 88, in dumps\n    cp.dump(obj)\n  File \"/Users/luweizheng/anaconda3/envs/dispy/lib/python3.11/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 733, in dump\n    return Pickler.dump(self, obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/types.pxi\", line 1710, in pyarrow.lib.PyExtensionType.__reduce__\nNotImplementedError: Please implement UnknownExtensionType.__reduce__"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class TorchPredictor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = torch.nn.Identity()\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        inputs = torch.as_tensor(batch[\"data\"], dtype=torch.float32)\n",
    "        with torch.inference_mode():\n",
    "            batch[\"output\"] = self.model(inputs).detach().numpy()\n",
    "        return batch\n",
    "\n",
    "ds = (\n",
    "    ray.data.from_numpy(np.ones((32, 100)))\n",
    "    .map_batches(TorchPredictor, compute=ray.data.ActorPoolStrategy(size=2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dispy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
